#configuration of the experiments

transformer:
  input_dim: 9
  num_classes: 3
  n_head: 4
  nlayers: 3

pretraining:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 300
  gpus: 0
  out_dim: 14
  pred_hidden_dim: 14
  proj_hidden_dim: 14
  num_ftrs: 64

finetuning:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 100









