#configuration of the experiments

transformer:
  input_dim: 13
  num_classes: 6
  n_head: 4
  nlayers: 3

pretraining:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 2
  gpus: 0
  out_dim: 14
  pred_hidden_dim: 14
  proj_hidden_dim: 14
  num_ftrs: 64

finetuning:
  batch_size: 256
  learning_rate: 0.0016612
  epochs: 2









